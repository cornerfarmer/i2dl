{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semantic Segmentation \n",
    "============\n",
    "\n",
    "In this exercise you are going to work on a computer vision task called semantic segmentation. In comparison to image classification the goal is not to classify an entire image but each of its pixels separately. This implies that the  output of the network is not a single scalar but a segmentation with the same shape as the input image. Think about why you should rather use convolutional than fully-connected layers for this task!\n",
    "\n",
    "Since we already introduced the basics of PyTorch and how to train a NN we leave the model design and architecture as well as the training up to you. We only provide you with the train, validation and test dataset and recommend you to look for inspirational, existing PyTorch implementations. Due to the fairly small size of the segmentation dataset you should not train a model from scratch but consider to (at least partially) finetune weights of an already exsisting model.\n",
    "\n",
    "The infamous  [Fully Convolutional Networks for Semantic Segmentation](https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf) paper might help you with finding a suitable segmentation model and architecture.\n",
    "\n",
    "<img src=\"https://camo.githubusercontent.com/d10b897e15344334e449104a824aff6c29125dc2/687474703a2f2f63616c76696e2e696e662e65642e61632e756b2f77702d636f6e74656e742f75706c6f6164732f646174612f636f636f7374756666646174617365742f636f636f73747566662d6578616d706c65732e706e67\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from exercise_code.data_utils import SegmentationData, label_img_to_rgb\n",
    "from torchvision import transforms\n",
    "#torch.set_default_tensor_type('torch.FloatTensor')\n",
    "#set up default cuda device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSRC-v2 Segmentation Dataset\n",
    "================\n",
    "\n",
    "Make yourself familiar with the segmentation dataset and how we implemented the `SegmentationData` class in `exercise_code/data_utils.py`. Furthermore have a look at the labels described in `data/segmentation_data/info.html`. Especially note the label `unlabeled`. Pixels with the label `unlabeled` should neither be considered in your loss nor in the accuracy of your segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "transforms_train = transforms.Compose([    \n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.Resize(500)\n",
    "])\n",
    "\n",
    "transforms_val = transforms.Compose([\n",
    "    #transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.Resize(384, 0)\n",
    "])\n",
    "\n",
    "train_data = SegmentationData(image_paths_file='datasets/segmentation_data/train.txt', transforms=transforms_val, transform_normalizer=normalize)\n",
    "val_data = SegmentationData(image_paths_file='datasets/segmentation_data/val.txt', transforms=transforms_val, transform_normalizer=normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 276\n",
      "Validation size: 59\n",
      "Img size:  torch.Size([3, 384, 384])\n",
      "Segmentation size:  torch.Size([384, 384])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk8AAANeCAYAAAALOurkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvAOZPmwAAHdNJREFUeJzt3X+w7Xtd1/H3Gw8BBnoxbyT4gwol9Q/TpvjDpDMDkZY0OQ6SDVNiFNg0Fv7RT/Su61RQ0VBNzuBIUw44pbccbiZow4ybsiINrEFtoK5CVyUEgSsIowKf/lhrX9fdnH3Oeu39Xeu7fjweM3fOPnutvdZnrb3Pdz3v+/td391jjAIAYDOPmnsBAACHRDwBAATEEwBAQDwBAATEEwBAQDwBAATEEzvX3T/T3TfnXgfAdXX3n+/uH5p7HexWO8/Taejud1XVi8YYb9ry/Syq6mljjBds836A49TdH1n766dX1a9X1SdWf3/xGOP7driWx1bVx6rq88YYv7Cr+2X/3Zh7AQBwbozx+POPr/s/fd19Y4zx8anWBufstjtB3f1N3f3j3f3K7v5gd/98d3/N2uVn3f3y7v6J7v7V7r6/uz9rddnN7v6FC7f3ru5+dnd/dVX9rap6fnd/pLv/5yX3/67ufvbq40V339fdr+vuD3f327v7i7r7b3b3L3f3g939nLWvfWF3/6/VdX+uu1984bb/Wne/p7t/qbtf1N2ju5+2uuwxq8f8f7v7vd396u5+3FTPK7B93f2V3f3fuvtDq3/nr+ruG6vLHrv6N/8t3f1AVf306vN/orv/9+pr/nF3v6W7X7B2my/u7nd09we6+4e7+ymri/7j6s93rLZpf+oW63lJd7/pwv2/pLsfWG0/X9bdT19tTx/q7u9bW+/d3f3G7n7f6r7v7+7PWbvtL+zu/7La3v1Id393d79m7fKvWnsu3tbdXznx080lxNPpekZVvaOqPruq/kFV/fPu7rXL/2xVfXNVfU5Vfbyq/umdbnCM8SNV9feq6vvHGI8fY3zZhmt5blW9tqqeWFU/VVU/WsufzadU1XdW1XevXfeXq+prq+ozquqFVfWq7v6KqqpVvH1bVT27qp5WVTcv3M8rquqLqur3ry5/SlV9x4ZrBPbDb1bVX66q31FVX1XL7ceLLlzna6vqD1TVl69i5Pur6qVVdXdV/dLqsqqq6u7nV9VfXd3Ok2q5DXrd6uJnrv58+mqb9voN1/isqvqyqvojVXVPLbefz6uq311Vf6iqvn51vUdV1aur6vNXl1VVvWq1rq6qH6iqH1s91ldU1XrwPbWqXl9Vf7uqPquqXlZVr+/uJ264Rq5BPJ2ud48xvmeM8Ymq+t5aRtKT1i5/7Rjjp8cYv1ZV315V39Ddn7altfynMcaPrsbr99VyA/eKMcZvVtW/rqqndvddVVVjjB8eYzwwlt5cVf+hlhvQqqpvqKp/Mcb4mTHGR6tqcX4Hqw3RX6yql44xPjDG+HAtQ+9Pb+kxAVswxviJMcZPjjE+McZ4oKpeU8tIWfd3xxgfGmN8rJZR9JNjjH+/2qa8sqo+uHbdl1TV3xljvHN1+b1V9Ye7+0l1da8YY3xkjPFTVfXOqnrDGOPdY4wP1HKb9eWrx/LeMcb9Y4yPjTEeqqqXrz2WL6yq31dV3znG+I0xxllVvXHtPv5cVf3gGONNY4xPjjHeUFU/W1XPKbbOMU+n6/+dfzDG+Ohq6PT4tcsfXPv43VX16FpOqbbhvWsff6yq3r+KuvO/n6/tQ6vdi/fUcoL0qFoeUPr21XWeXFX/fe221h/D3avrvnVtwNZVta0gBLagu7+kqv5RVX1FVT2ulq9j//nC1db/7T95/e9jjE929y+uXf4FVfXq7v6utc99vKo+t6oeuuIyL27TLv798VVV3f2EqvontZyW37W6/PxQgidX1fvGGL++9rUPVtUT1tb9jd39vLXLH736OrbM5InLfN7ax59fy1H5+6vq12oZIVVVtZpG3b123a29fbO7H1NV/7aW/+f4pDHGXVX1hlpGUFXVe2q5wTu3/hjeX8uN1peOMe5a/feZ6wenAgfhe6rqbVX1e8cYn1HLXft94Trr26FHbBe6+/yQgHMPVtU3rW0X7hpjPG6M8dba4vZs5W+s1vYHV4/lOfXI7dndq+3eufVt2oNV9ZoL6/7tY4xXbXnNlHjici/o7i/p7k+v5cbp36ymQe+sqseuDsB8dC33s6//435vLXezbeNn67et7ut9VfXx1RRqfUT9A1X1wu7+4tW6v/38gjHGJ2u50X1Vd//Oqqrufkp3/7EtrBPYnidU1UNjjI9095dW1V+4w/X/XVU9o7v/+OpA7W+r5fGV515dVS/r7qdXVXX3E7v766uqVlOfh6rq90z9IFaeUFUfreVU/bNruT09985aHpf6su5+dHc/s6q+eu3y762q53X3s7r707r7cauPf9eW1soa8cRlXltV/7KWu/ceW1XfWlW12i//l2p5nMEv1nIStf7uu/tWf/5Kd79tygWtjlP61lpG0ger6s/UcsN4fvkba3lg5o9V1f+pqresLjofe//18893969W1Zuq6ulTrhHYupdW1Yt6eT6o76rlweCXGmO8p6q+sZbbhvfXctLz9lptF8YY/6qq/llV/eBqu/A/quqPrt3Ed1TVfat3tP3JiR/LK2t5OMSvVNWP13KSfr7uUVXPr+UuvQ/W8p3M962t++dqeeD5vavH9e6q+ivldX0nnCSTT9HdZ1X1ujHGa+503X3W3V9cy7cqP8a5XoCq5bmfavk/hc8dY/zXudeT6O77q+otY4yXz72WU6dQOSrd/XWr8zk9sar+flX9kHCC09bdX9Pdn9nLM4bfU8tdZW+deVl31N3P6O6ndvejuvu5tdxtd//c60I8cXxeXMtzQT1Qy1/p8C3zLgfYA8+sqp+v5bbhWVX1dWOM35h3SRv53FruzvtwVf3DqvrmMcbPzrskquy2AwCImDwBAAScJBNgQt1tnA8Haoxx8Zxht2TyBAAQEE8AAAHxBAAQEE8AAAHxBAAQEE8AAAHxBAAQEE8AAAHxBAAQEE8AAAHxBAAQEE8AAAHxBAAQEE8AAAHxBAAQEE8AAAHxBAAQEE8AAAHxBAAQEE8AAAHxBAAQEE8AAAHxBAAQEE8AAAHxBAAQEE8AAAHxBAAQEE8AAAHxBAAQEE8AAAHxBAAQEE8AAAHxBAAQEE8AAAHxBAAQEE8AAAHxBAAQEE8AAAHxBAAQEE8AAAHxBAAQEE8AAAHxBAAQEE8AAAHxBAAQEE8AAAHxBAAQEE8AEztbLOZeArBFPcaYew0AR+PN9957x43qTXEFe2mM0Ztcz+QJACBwY+4FAJyai7v19mUSZXfjPG4uFrf8mThbLG7757nLvvbi7RyL6/ycTvU82G0HMKFNdtvdzmUvdrd7cT2//FYvwufXvdVtwEW3Cq+LH1/1Nq66jvO/V033M3xZRNltBwCwBSZPABO67uQJmMfNxcLkCQBgG8QTAHDykuOpxBMAQEA8AQAExBMAQEA8AQAExBMAQEA8AQAExBMAQEA8AQAExBMAQEA8AQAExBMAQEA8AQAExBMAQEA8AQAExBMAQEA8AQAExBMAQEA8AQAExBMAQEA8AQAExBMAQEA8AQAExBMAQEA8AQAExBMAQEA8AQAExBMAQEA8AQAExBMAQEA8AQAExBMAQEA8AQAExBMAQEA8AQAExBMAQEA8AQAExBMAQEA8AQAExBMAQEA8AQAExBMAQEA8AQAExBMAQEA8AQAExBMAQEA8AQAExBMAQEA8AQAExBMAQEA8AQAExBMAQEA8AQAExBMAQEA8AQAExBMAQEA8AQAExBMAQEA8AQAExBMAQEA8AQAExBMAQEA8AQAExBMAQEA8AQAExBMAQEA8AQAExBMAQEA8AQAExBMAQEA8AQAExBMAQEA8AQAExBMAQEA8AQAExBMAQEA8AQAExBMAQEA8AQAExBMAQEA8AQAn7+ZisfF1xRMAQEA8AQAnLZk6VYknAICIeAIACIgnAOBkpbvsqsQTAHCirhJOVeIJACAingAAAuIJACAgngCAk3TmmCcAgO0TTwAAAfEEABAQTwAAAfEEABAQTwDAybrKO+7EEwBAQDwBACfLLwYGANgy8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAMBJusppCqrEEwBA5MbcCwAA2KWrTpzOmTwBAATEEwBAQDwBAAQc8wQAnITrHut0zuQJADh6U4VTlXgCAIiIJwCAgGOeAICjM+VuuovEEwBwNLYZTefstgMACJg8AXCQbi4WdbaaMpxPG852MHVgPruYKm3C5AkAINBjjLnXAHA03nzvvTaqE9jVhOHipOriBOt8unXZn5t87cWP73TZZZ+73ec3daf7vd19Xfe+129nX40xepPriSeACYmnq9vnF1VOw6bx5JgnAHZOKHHIxBMAOyGYOBbiCYCN3en4HIHEKXDME8CEDuWYp9sdkAynyjFPAAdkjmgRSnA14glgRgIGDo/ddgAT6u6HN6oXz5ED7LdNd9s5wzgAQMDkCWBC65Mn4LCYPAEAbIF4AgAIiCcAgIB4AgAIiCcAgIB4AgAIiCcAgIB4AgAIiCcAgIB4AgAIiCcAgIB4AgAIiCcAgIB4AgAIiCcAgIB4AgAIiCcAgIB4AgAIiCcAgIB4AgAIiCcAgIB4AgAIiCcAgIB4AgAIiCcAgIB4AgAIiCcAgIB4AgAIiCcAgIB4AgAIiCcAgIB4AgAIiCcAgIB4AgAIiCcAgIB4AgAIiCcAgIB4AgAIiCcAgIB4AgAIiCcAgIB4AgAIiCcAgIB4AgAIiCcAgIB4AgAIiCcAgIB4AgAIiCcAgIB4AgAIiCcAgIB4AgAIiCcAgECPMeZeAwDAwTB5AgAIiCcAgIB4AgAIiCcAgIB4AgAIiCcAgIB4AgAIiCcAgIB4AgAIiCcAgIB4AgAIiCcAgIB4AgAIiCcAgIB4AgAIiCcAgIB4AgAIiCcAgIB4AgAIiCcAgIB4AgAIiCcAgIB4AgAIiCcAgIB4AgAIiCcAgIB4AgAIiCcAgIB4AgAIiCcAgIB4AgAIiCcAgIB4AgAIiCcAgIB4AgAIiCcAgIB4AgAIiCcAgIB4AgAIiCcAgIB4AgAIiCcAgIB4AgAIiCcAgIB4AgAIiCcAgIB4AgAI3Jh7AQDHpLvH3GsArmaM0Ztcz+QJACAgngAAAuIJACAgngAAAuIJACDg3XYAE1os5l4BsG0mTwAAAfEEABAQTwAAAfEEABAQTwAAAfEEABAQTwAAAfEEABAQTwAAAfEEABAQTwAAAfEEABAQTwAAAfEEABAQTwAAAfEEABAQTwAAAfEEABAQTwAAAfEEABAQTwAAAfEEABAQTwAAAfEEABAQTwAAAfEEwM4tFp/68WJx648vu87tPn+7+9vka+90e5y2HmPMvQaAo3HvvW2juoFDDJNDXDOZe+4Zvcn1bmx7IQBwDO4UT+LqdNhtBwAQMHkCYKeOdUJz2eM61sd7ykyeAGCLHHx+fMQTAEDAbjsAduaUJzB26x0PkycAgIDJEwA7YcJya7c7gSf7STwBwJ4RVPtNPAGwVVO86J/tWTncnGE9679OhnmJJwD2yr6F0q1cXOMuY+pWv4eP3fK77QAm5Hfb/Zb0Rf0QomkTc0ylqkTUFPxuOwBmk7yQH0s0nVt/PHOFFNslngCYxbFF062cP0YRdVzEEwCT2aQRTiGaLhJRx8VJMgG4tk1/f9sphtO6bT5+v0Nvd8QTAEDAu+0AJnQq77Y75QPCp7aNXXme8qvxbjsAJieawG47AICIyRMAt3SdwZGp0+acF+rwmDwBAARMngCYjInTfnDagu0STwBcm2iahpNpHga77QC4FuE0vSmeU9+W7RFPAFyZcNoez+3+Ek8AcKT013aIJwCuxGRk+84WC8/zHhJPADyCd2rtHwG1X8QTADEv5rt31efct2p64gmAiHCaj+d+P4gnAICAeAJgYyYf8/M9mJ94AoAjp7emJZ4AAAJ+tx0Ad2RX0WHz7ZuWyRMAQEA8AfAwE4rDkJ553Pd1WuIJgId5kYU7E08AcORE8bTEEwAcOfE0LfEEABAQTwA8zITiOPm+Tks8AQAExBMAHDmTp2mJJwA4cuJpWuIJACAgngDgQG16lnGTp2mJJwCqygssbEo8AQAExBMAHLD0lwRzfeIJACAgngAAAuIJACAgngDgBDgsajriCQAvrBAQTwBwAgTydMQTAEBAPAGcOBOJ43Cn8z35Pk9HPAHACRBP0xFPACfMC+rp8L2ejngCAAiIJ4ATZRJxWny/pyOeAAAC4gkAICCeAOAE2G03HfEEABC4MfcCAIDru2m0tDMmTwAAAfEEcIIMKeDqxBPACRJPcHXiCeAEiafT5Ps+DfEEABAQTwAAAfEEABAQTwAAAfEEABBwhnGAE3T+rivvvjp8ziy+eyZPACfM6+5hu0o4LRa+79clngAAAuIJ4MRtMom4uVjYPQQr4gkAICCeAKiqzY6DMX3aDyaB8xJPAHBARNP8xBMADzN92m+e+/0gngB4hE0PIGe3POf7QzwBAATEEwBXYhKyO57r/SKeAGCPCaf9I54AuCUHj8/P87ufxBMA1+IFnlMjngAAAjfmXgAA++t8qLTpqQvOTKEmYZq338QTAJMRUdcjmg6D3XYAAAHxBMDkTFAyc/yiX9+iqxNPAAAB8QQAMzKlOzziCYCtmGNXFOyCd9sBwAyE5eEyeQJgq0ygPpXn47CJJwB2QjAIyWMhngDYmVMNh32NpsXCKQuuQjwBsFP7GBHbdGqP9xQ4YByAnTuFX+NyCNF0AEvcSyZPAMzmEAIjta+76JiOeAIACNhtB8CldjFAuTil2eddecc0UTqih7Jz4gmAvXJzsdibgNp1LK0/7mMKtWNjtx0AQMDkCYBbmnPwcWhTl00nZcm7DC+7zqE9N8fI5AkAIGDyBABXlB6bNcWxXGdOhTA78QTAyblTxGwSJ3Me1H5+3yJqHuIJgEc4tNfjbUTMvrzbj/0kngB42L41g4hhH4knAKpqN+Ekhqbl+Kd5iCcAthJOQoljJZ4ATti2+kY4cczEE8AJEk1wdeIJ4ERsu2uE0zyctmD3nGEcACBg8gRw4PZh4GDqxCkxeQI4YJoFdk88AQAExBPAgTJ14jr8/FydeAIACIgngAO0T1MDB4sfJt+2qxNPAAdksdi/Fz3nF+LUiCcAgIDzPAEcAMMd2B8mTwB7TjjBfhFPAFyb4544JXbbAewhLQL7SzwB7BHRBPtPPAHsAdHErvhZuz7HPAEABEyeAHbA/+2zD/wcTsPkCQAgIJ4AAAJ22wFMzK4ROG4mTwAAAfEEMCFTJzh+4gkAICCeAOAEmIpORzwBwAkQT9MRTwBwAsTTdMQTAJwA8TQd8QQAJ0A8TUc8AcAJEE/TEU8AAAHxBAAQ8LvtAOCA3bQ/budMngCYhBdxToV4AgAIiCcAOFDptM9wcBriCQAgIJ4AAALiCYDJ3FwsHDi+5xYLu++uy6kKAODACNR5mTwBwAERTvMTTwBMzgv89OwS3R89xph7DQBHo7ttVC8484J/LdsMJt+aR7rnntGbXE88AUxIPG3mYlBdNxCOJdDmmizt29O3vp5drk08AcxAPHGI9imebreWba9TPAHMQDxxyOaOqOT+t7HWTePJAeMAAAGTJ4AJmTxxDHY9gZri/qa4DbvtAGYgnjgm246oXUTarQ4+v9Wfi0XVGHbbAQBMzuQJYEImTxyz60yK5j4YfRMmTwAAW2DyBDAhkydO1cXjiA7RppMn8QQwIfEEh8tuOwCALRBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAACBHmPMvQYAgINh8gQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAELgx9wIAjkl3j7nXAFzNGKM3uZ7JEwBAQDwBAATEEwBAQDwBAATEEwBAQDwBAATEEwBAQDwBAATEEwBAQDwBAATEEwBAQDwBAATEEwBAQDwBAATEEwBAQDwBAATEEwBAQDwBAATEEwBAQDwBAATEEwBAQDwBAATEEwBAQDwBAATEEwBAQDwBAATEEwBAQDwBAATEEwBAQDwBAATEEwBAQDwBAATEEwBAQDwBAATEEwBAQDwBAATEEwBAQDwBAATEEwBAQDwBAATEEwBAQDwBAATEEwBAQDwBAATEEwBAQDwBAATEEwBAQDwBAATEEwBAQDwBAATEEwBAQDwBAATEEwBAQDwBAATEEwBAQDwBAATEEwBAQDwBAATEEwBAQDwBAATEEwBAQDwBAATEEwBAQDwBAATEEwBAQDwBAATEEwBAQDwBAATEEwBAQDwBAATEEwBAQDwBAATEEwBAQDwBAATEEwBAQDwBAATEEwBAQDwBAATEEwBAQDwBAARuzL0AAI7P2WJx5a+9eY2vhV3oMcbcawA4Gt19shvV6wTTZYQUuzTG6E2uZ7cdAEDA5AlgQqc0edrGpGlfnE+8Lj7Gm4vFw5+77OOLt7Gv7Fr9VCZPAABbYPIEMKFTmTwd89Rp2+ac2jgu7fY2nTyJJ4AJnUI8CafdmDJKdvE9O4aIEk8AMzj2eBJO3MkhR5R4ApjBscaTaCJxqAHlgHEAYBbHHtviCQCY3DEHlHgCALbiWAPK77YD4FLH+uIH12HyBABszTEGuHgCAAiIJwBu6RgnBszj2H6WxBMAQEA8AQAExBMAsHVni8XR7L4TTwAAAfEEABAQTwAAAfEEABAQTwAAAfEEABAQTwAAAfEEABAQTwAAAfEEABAQTwAAAfEEABAQTwAAAfEEABAQTwAAAfEEABAQTwAAAfEEABAQTwAAAfEEABAQTwDAzpwtFnMv4drEEwBAQDwBAATEEwBAQDwBAATEEwBAQDwBAATEEwBAQDwBAATEEwBAQDwBAATEEwBAQDwBAATEEwBAQDwBADt1tljU2WIx9zKuTDwBAATEEwBAQDwBAATEEwBA4MbcCwAATsvNAz5YvMrkCQAgIp4AAALiCYBbOvRdK7At4gkAICCeALjUzcXCBAouEE8AAAHxBADszDFMMsUTAEBAPAEABMQTAHfkwHH4LT3GmHsNAEeju49+o3omoriCQ4jvMUZvcj2TJwAih/AiCNskngCICSgSx/bzYrcdwIROYbfdRXbjcTuHFE6b7rYTTwATOsV4OieiuOiQwqnKMU8AAFth8gQwoVOePG1i29Opm4vFp9zHrT53u89f576rLn+MU9/fvjq0adM6u+0AZiCeOCTbiMdDZrcdAMAWmDwBTMjkiWNzdkJnlzd5AgDYApMngAmZPMHhMnkCANgC8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEOgxxtxrAAA4GCZPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAAAB8QQAEBBPAACB/w/ntw4+v1rEDgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4b06e129b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Train size: %i\" % len(train_data))\n",
    "print(\"Validation size: %i\" % len(val_data))\n",
    "print(\"Img size: \", train_data[0][0].size())\n",
    "print(\"Segmentation size: \", train_data[0][1].size())\n",
    "\n",
    "num_example_imgs = 3\n",
    "plt.figure(figsize=(10, 5 * num_example_imgs))\n",
    "for i, (img, target) in enumerate(train_data[:num_example_imgs]):\n",
    "    # img\n",
    "    plt.subplot(num_example_imgs, 2, i * 2 + 1)\n",
    "    #plt.imshow(img.numpy().transpose(1,2,0))\n",
    "    plt.axis('off')\n",
    "    if i == 0:\n",
    "        plt.title(\"Input image\")\n",
    "    \n",
    "    # target\n",
    "    plt.subplot(num_example_imgs, 2, i * 2 + 2)\n",
    "    plt.imshow(label_img_to_rgb(target.numpy()))\n",
    "    plt.axis('off')\n",
    "    if i == 0:\n",
    "        plt.title(\"Target image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.segmentation_nn import SegmentationNN\n",
    "def generate_cache():\n",
    "    model = SegmentationNN(384, mode=1)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    train_cache = []\n",
    "    for img, target in train_data:\n",
    "        cache = model.forward(img.unsqueeze(0))[0]\n",
    "        train_cache.append((cache, target))\n",
    "\n",
    "    val_cache = []\n",
    "    for img, target in val_data:\n",
    "        cache = model.forward(img.unsqueeze(0))[0]\n",
    "        val_cache.append((cache, target))\n",
    "    return train_cache, val_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('cache-384-100.pkl', 'wb') as handle:\n",
    "    pickle.dump(generate_cache(), handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('cache-384-100.pkl', 'rb') as handle:\n",
    "    train_cache, val_cache = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Design and Train your Network\n",
    "==========\n",
    "\n",
    "Implement your network architecture in `exercise_code/classifiers/segmentation_nn.py` and adapt your `Solver` to process segmentation labels. To compensate for the dimension reduction of a typical convolution layer, you should probably include a `nn.Upsample` layer near the end of your network. Also remember to consider finetuning a model instead of training it from scratch.\n",
    "\n",
    "Up until now we only used the default loss function (`nn.CrossEntropyLoss`) of our `Solver` class. However, in order to ignore the `unlabeled` pixels for the computation of our loss, we have to use a customized version of the loss for the initializtation of the `Solver` class. The `ignore_index` argument of the loss can be used to filter the `unlabeled` pixels and computes the loss only over remaining pixels.\n",
    "\n",
    "Step by step:\n",
    "1. Initialize training and validation data loaders.\n",
    "2. Design and initialize a convolutional neural network architecture that has input (N, C, H, W) and output (N, num_classes, H, W) and is based on an already pretrained network.\n",
    "3. Initialize a solver with a loss function that considers the `unlabeled` pixels.\n",
    "4. Adjust the logging of your solver to account for the `unlabeled` pixels.\n",
    "5. Train a segmentation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: [torch.Size([23, 4096, 1, 1]), torch.Size([23]), torch.Size([23, 23, 64, 64])]\n",
      "START TRAIN.\n",
      "[Iteration 5/345] TRAIN loss: 3.230\n",
      "[Iteration 10/345] TRAIN loss: 3.210\n",
      "[Iteration 15/345] TRAIN loss: 3.141\n",
      "[Iteration 20/345] TRAIN loss: 3.132\n",
      "[Iteration 25/345] TRAIN loss: 3.025\n",
      "[Iteration 30/345] TRAIN loss: 2.970\n",
      "[Iteration 35/345] TRAIN loss: 3.016\n",
      "[Iteration 40/345] TRAIN loss: 3.035\n",
      "[Iteration 45/345] TRAIN loss: 2.859\n",
      "[Iteration 50/345] TRAIN loss: 2.876\n",
      "[Iteration 55/345] TRAIN loss: 2.830\n",
      "[Iteration 60/345] TRAIN loss: 2.786\n",
      "[Iteration 65/345] TRAIN loss: 2.822\n",
      "[Epoch 1/5] TRAIN acc/loss: 0.276/2.822\n",
      "[Epoch 1/5] VAL   acc/loss: 0.331/2.683\n",
      "[Iteration 74/345] TRAIN loss: 2.669\n",
      "[Iteration 79/345] TRAIN loss: 2.572\n",
      "[Iteration 84/345] TRAIN loss: 2.626\n",
      "[Iteration 89/345] TRAIN loss: 2.692\n",
      "[Iteration 94/345] TRAIN loss: 2.460\n",
      "[Iteration 99/345] TRAIN loss: 2.618\n",
      "[Iteration 104/345] TRAIN loss: 2.560\n",
      "[Iteration 109/345] TRAIN loss: 2.669\n",
      "[Iteration 114/345] TRAIN loss: 2.513\n",
      "[Iteration 119/345] TRAIN loss: 2.231\n",
      "[Iteration 124/345] TRAIN loss: 2.326\n"
     ]
    }
   ],
   "source": [
    "from exercise_code.segmentation_nn import SegmentationNN\n",
    "from exercise_code.solver import Solver\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=4, shuffle=True, num_workers=1)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=1, shuffle=False, num_workers=1)\n",
    "\n",
    "model = SegmentationNN(384)\n",
    "model.to(device)\n",
    "#model.eval()\n",
    "#out = model.forward(next(iter(train_loader))[0])\n",
    "solver = Solver(optim_args={\"lr\": 1e-4, \"weight_decay\": 0, \"momentum\": 0.9}, optim=torch.optim.SGD,\n",
    "               loss_func=torch.nn.CrossEntropyLoss(size_average=True, ignore_index=-1))\n",
    "solver.train(model, train_loader, val_loader, log_nth=5, num_epochs=5, parameters=model.parameters_to_train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 23, 384, 384])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.argmax(np.squeeze(np.squeeze(out.detach().numpy(), -1)), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12,  1,  1, ..., 11, 16, 21],\n",
       "       [11, 14, 21, ..., 12, 22, 10],\n",
       "       [18,  1, 20, ..., 16,  6, 21],\n",
       "       ...,\n",
       "       [ 0,  5, 21, ..., 12,  6,  9],\n",
       "       [21, 18,  5, ...,  6,  4, 17],\n",
       "       [20, 17,  2, ..., 15,  5, 15]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(out.detach().numpy()[2], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test your Model\n",
    "\n",
    "Your model should easily yield a pixel accuracy of more than 60%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-2:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/domin/virtualenv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f39719811d0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/domin/virtualenv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 347, in __del__\n",
      "    def __del__(self):\n",
      "  File \"/home/domin/virtualenv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 178, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 10942) exited unexpectedly with exit code 1.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-a2472d928826>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtargets_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/i2dl/exercise_4/exercise_code/segmentation_nn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;31m########################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0morig_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_ft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_conv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenv/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenv/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 301\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_data = SegmentationData(image_paths_file='datasets/segmentation_data_test/test.txt', transforms=transforms_val, transform_normalizer=normalize)\n",
    "test_loader = torch.utils.data.DataLoader(test_data,\n",
    "                                          batch_size=1,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=1)\n",
    "\n",
    "test_scores = []\n",
    "model.eval()\n",
    "for inputs, targets in test_loader:\n",
    "    inputs, targets = inputs.to(device), targets.to(device)\n",
    "    \n",
    "    outputs = model.forward(inputs)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    targets_mask = targets >= 0\n",
    "    test_scores.append(np.mean((preds == targets)[targets_mask].data.cpu().numpy()))\n",
    "    \n",
    "model.train()\n",
    "np.mean(test_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1440 with 12 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_data = SegmentationData(image_paths_file='datasets/segmentation_data_test/test.txt', transforms=transforms_val, transform_normalizer=normalize)\n",
    "test_loader = torch.utils.data.DataLoader(test_data,\n",
    "                                          batch_size=1,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=1)\n",
    "num_example_imgs = 4\n",
    "plt.figure(figsize=(15, 5 * num_example_imgs))\n",
    "for i, (img, target) in enumerate(test_data[:num_example_imgs]):\n",
    "    inputs = img.unsqueeze(0)\n",
    "    inputs = inputs.to(device)\n",
    "    \n",
    "    outputs = model.forward(inputs)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    pred = preds[0].data.cpu()\n",
    "    \n",
    "    img, target, pred = img.numpy(), target.numpy(), pred.numpy()\n",
    "    \n",
    "    # img\n",
    "    plt.subplot(num_example_imgs, 3, i * 3 + 1)\n",
    "    plt.axis('off')\n",
    "    #plt.imshow(img.transpose(1,2,0))\n",
    "    if i == 0:\n",
    "        plt.title(\"Input image\")\n",
    "    \n",
    "    # target\n",
    "    plt.subplot(num_example_imgs, 3, i * 3 + 2)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(label_img_to_rgb(target))\n",
    "    if i == 0:\n",
    "        plt.title(\"Target image\")\n",
    "\n",
    "    # pred\n",
    "    plt.subplot(num_example_imgs, 3, i * 3 + 3)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(label_img_to_rgb(pred))\n",
    "    if i == 0:\n",
    "        plt.title(\"Prediction image\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Filter Weights\n",
    "You can visualize the convolutional filters of the very first layer by running the following cell. The kernels should exhibit clear structures of differently oriented edges, corners and circles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.vis_utils import visualize_grid\n",
    "\n",
    "# first (next) parameter should be convolutional\n",
    "conv_params = next(model.parameters()).data.cpu().numpy()\n",
    "grid = visualize_grid(conv_params.transpose(0, 2, 3, 1))\n",
    "plt.imshow(grid.astype('uint8'))\n",
    "plt.axis('off')\n",
    "plt.gcf().set_size_inches(6, 6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model\n",
    "\n",
    "When you are satisfied with your training, save the model for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"models/segmentation_nn.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible Next Steps\n",
    "\n",
    "1. Implement and integrate a task specific metric such as [Intersection over Union (IoU)](http://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/)\n",
    "2. Hyperparameter optimization\n",
    "3. Data augmentation ([PyTorch tutorial](http://pytorch.org/tutorials/beginner/data_loading_tutorial.html))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
